{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71ae8b0",
   "metadata": {},
   "source": [
    "# Regression Models for Fa Prediction using Descriptors Calculated with Mordred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf98e1",
   "metadata": {},
   "source": [
    "## Materials and Method\n",
    "\n",
    "- Libraries: NumPy, pandas, scikit-learn, matplotlib, RDKit, mordred and SHAP\n",
    "- Dataset: Fraction of absorption (Fa) and Parmeability measured by Caco-2 cells (Papp), which were collected previous strudy (Esaki, et al., Journal of Phermeceutical Sciences, 2019)\n",
    "- Descriptor calcularion: Mordred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb316c",
   "metadata": {},
   "source": [
    "### Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('numpy version: ', np.__version__)\n",
    "print('pandas version: ', pd.__version__)\n",
    "print('matplotlib version: ', matplotlib.__version__)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold,  SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "print('scikit-learn version: ', sklearn.__version__)\n",
    "\n",
    "from rdkit import Chem, rdBase\n",
    "print('rdkit version: ', rdBase.rdkitVersion)\n",
    "\n",
    "import shap\n",
    "print('shap version: ', shap.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Papp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d03d9",
   "metadata": {},
   "source": [
    "### Dataset Import\n",
    "\n",
    "The dataset contained information on the chemical structure of 5567 compounds as SMILES strings. In this dataset, the number of Fa experimental values was 946, respectively. Owing to its accuracy, we used CORINA (ver. 4.4.0) to generate 3D structures of the chemical compounds as structure data format (SDF) .\n",
    "\n",
    "We used the molecular descriptor calculator, Mordred, to calculate descriptors (1826 for 1D, 2D, and 3D). These descriptors were calculated for Papp dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3Ddescriptors = pd.read_csv('CBIJ_Esaki_et_al_Descriptor_Mordred_1D2D3D.csv', index_col=0)\n",
    "df_3Ddescriptors.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eb96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3Ddescriptors = df_3Ddescriptors.drop(\"Fa\", axis=1)\n",
    "df_3Ddescriptors.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43283e6",
   "metadata": {},
   "source": [
    "## Definition of Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4284157",
   "metadata": {},
   "source": [
    "### Data preparation and Distribution conformation\n",
    "\n",
    "The Papp dataset was randomly split into training (70%, 3087 compounds) and test set (30%, 1324 compounds) using the train_test_split function in scikit-learn. The Papp measurements ranged between 0.00016 and 880. We transformed these values to log10(Papp) to scatter the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_separation(df, target):\n",
    "    df = df.dropna(subset=[target])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 2:], df[target],\n",
    "                                                        train_size=0.7, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def distribution_conformation(df, target):\n",
    "    X_train, X_test, y_train, y_test = data_separation(df, target)\n",
    "    \n",
    "    # remove descriptors with nan\n",
    "    X_train = X_train.dropna(axis=1)\n",
    "    X_test = X_test[X_train.columns]\n",
    "    print(X_train.shape, X_test.shape, X_test.dropna(axis=1).shape)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 3))\n",
    "    \n",
    "    for f in range(4):\n",
    "        ax = fig.add_subplot(1, 4, f+1)\n",
    "        if f == 0:\n",
    "            ax.hist(y_train, bins=25, label='Training set')\n",
    "            ax.hist(y_test, bins=25, label='Test set')\n",
    "            ax.set_title('{}'.format(target))\n",
    "        elif f == 1:\n",
    "            if target == 'Fa':\n",
    "                y_train[y_train == 0] = 0.01\n",
    "                y_train[y_train == 1] = 0.99\n",
    "                y_train = np.log(y_train/(1-y_train))\n",
    "                y_test[y_test == 0] = 0.01\n",
    "                y_test[y_test == 1] = 0.99\n",
    "                y_test = np.log(y_test/(1-y_test))\n",
    "            elif target == 'Papp':\n",
    "                y_train = np.log(y_train)\n",
    "                y_test = np.log(y_test)\n",
    "            ax.hist(y_train, bins=25, label='Training set')\n",
    "            ax.hist(y_test, bins=25, label='Test set')\n",
    "            ax.set_title('Converted {}'.format(target))\n",
    "        elif f == 2:\n",
    "            ax.hist(X_train['MW'], range=(0,700), bins=25, label='Training set')\n",
    "            ax.hist(X_test['MW'], range=(0,700), bins=25, label='Test set')\n",
    "            ax.set_title('MW: {} data'.format(target))\n",
    "        elif f == 3:\n",
    "            ax.hist(X_train['SLogP'], range=(-10,10), bins=25, label='Training set')\n",
    "            ax.hist(X_test['SLogP'], range=(-10,10), bins=25, label='Test set')\n",
    "            ax.set_title('SLogP: {} data'.format(target))\n",
    "        plt.legend(fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6e8cb",
   "metadata": {},
   "source": [
    "### Descriptor preparation\n",
    "\n",
    "We performed data preparation steps for descriptors of compounds in the training set. First, descriptors with nan were removed. Next, descriptors with small variance were removed using the VarianceThreshold function (threshold=1.0), and the retained descriptors were normalized using the StandardScaler function. Finally, effective descriptors for prediction were selected using ElasticNet. ElasticNetCV (cv=10, max_iter=1000000) selected proper alpha (penalty) and l1_ratio (norm ratio), using which we performed ElasticNet to retain proper descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptor_preparation(X_train, X_test, y_train):\n",
    "    # Remove no variance descriptors\n",
    "    var = VarianceThreshold(threshold=1.0).fit(X_train)\n",
    "    X_train = X_train.loc[:, var.get_support()]\n",
    "    X_test = X_test.loc[:, var.get_support()]\n",
    "    \n",
    "    # Standard Scaler\n",
    "    ss = StandardScaler().fit(X_train)\n",
    "    X_train_scd = pd.DataFrame(ss.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "    X_test_scd = pd.DataFrame(ss.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "    \n",
    "    # Fill in NaNs with avereges\n",
    "    train_averages = X_train_scd.mean()\n",
    "    X_test_scd = X_test_scd.fillna(train_averages)\n",
    "    \n",
    "    # Descriptor Selection\n",
    "    elastic = ElasticNetCV(cv=10, max_iter=1000000, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], n_jobs=-1).fit(X_train_scd, y_train)\n",
    "    print(f'alpha: {elastic.alpha_}, l1_ratio: {elastic.l1_ratio_}')\n",
    "    \n",
    "    elastic_model = ElasticNet(alpha=elastic.alpha_, l1_ratio=elastic.l1_ratio_, max_iter=1000000).fit(X_train_scd, y_train)\n",
    "    selected_descriptors = pd.DataFrame(elastic_model.coef_)\n",
    "    selected_descriptors.index = X_train_scd.columns\n",
    "    selected_descriptors = selected_descriptors[selected_descriptors[0] != 0]\n",
    "    X_train_elastic = X_train_scd.loc[:, selected_descriptors.index]\n",
    "    X_test_elastic = X_test_scd.loc[:, selected_descriptors.index]\n",
    "    print(f'number of selected descriptor: {X_train_elastic.shape[1]}')\n",
    "    \n",
    "    return X_train_elastic, X_test_elastic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85780cdd",
   "metadata": {},
   "source": [
    "### Model construction\n",
    "\n",
    "To construct Support Vector Regression (SVR) model for Papp prediction. SVR is a kernel function model where we employed the frequently used radial basis function kernel. Here, three parameters need to be optimized, which are C: regularization parameter, epsilon: insensitive zone, and gamma: kernel coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3089b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "def model_construction_svr(X_train_elastic, y_train):    \n",
    "    # SVR\n",
    "    search_params = [{'C'       : [2**n for n in range(0, 7, 1)],\n",
    "                      'epsilon' : [2**n for n in range(-5, 5, 1)],\n",
    "                      'gamma'   : [2**n for n in range(-10, 0, 1)]}]\n",
    "    gs_svr = GridSearchCV(SVR(kernel='rbf'),\n",
    "                          search_params,\n",
    "                          cv=10,\n",
    "                          n_jobs=-1,\n",
    "                          scoring='neg_mean_squared_error')\n",
    "    gs_svr.fit(X_train_elastic, y_train)\n",
    "    \n",
    "    return gs_svr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef752dc",
   "metadata": {},
   "source": [
    "### Visualization of results\n",
    "\n",
    "The R2 is an inadequate score for nonlinear models. Thus the root mean squared error (RMSE) were employed for comparing the predictive performance between the three algorithms.\n",
    "\n",
    "The scatter plot shows the result of the observed and predicted Papp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e49caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_evaluation(model, X_test, y_test):\n",
    "    #予測値\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f'Correlation coefficient between observed and predicted values: {np.corrcoef(y_pred, y_test)[0,1]}')\n",
    "    \n",
    "    #グラフのスケール設定\n",
    "    y_min = min([min(list(y_test)), min(list(y_pred))])\n",
    "    y_max = max([max(list(y_test)), max(list(y_pred))])\n",
    "    \n",
    "    ###実測値と予測値の散布図###\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    #散布図\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    #タイトル\n",
    "    plt.title(f'Scatter plot, RMSE: {round(np.sqrt(np.sum((y_pred - y_test)**2)/len(y_test)), 4)}', fontsize=14)\n",
    "    plt.xlabel('Observed value')\n",
    "    plt.ylabel('Predicted value')\n",
    "    \n",
    "    #グラフ間の距離の調整\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550df47a",
   "metadata": {},
   "source": [
    "## Results: Fa prediction using 1D2D3D descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05abba04",
   "metadata": {},
   "source": [
    "### Confirming distributions\n",
    "\n",
    "The distributions of this dataset in terms of molecular weight and slogp that were calculated using Mordred are shown in Figure 2 in body text. The p-values of the Wilcoxon signed rank test were higher than 0.05 in the converted Papp, MW, and slogp measurements, confirming that there was no bias in the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f253d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = distribution_conformation(df_3Ddescriptors, 'Papp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69540908",
   "metadata": {},
   "source": [
    "### Model Construction\n",
    "\n",
    "The suitable parameters were selected using GridsearchCV (cv=10, score=’neg-mean-squared-error')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5071f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_elastic, X_test_elastic = descriptor_preparation(X_train, X_test, y_train)\n",
    "\n",
    "svr_model = model_construction_svr(X_train_elastic, y_train)\n",
    "svr_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc692e8",
   "metadata": {},
   "source": [
    "### Performance evaluation\n",
    "\n",
    "The predicted Papp was calculated with SVR model constructed using 1D, 2D and 3D descriptor. The x-axis and y-axis show the values of converted Fa. The correlation coefficient scores between the observed and predicted values were 0.7019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb04ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "performance_evaluation(svr_model, X_test_elastic, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802468d3",
   "metadata": {},
   "source": [
    "### Contribution of descriptors for Papp prediction\n",
    "\n",
    "Although the generated relationship between the descriptors and predicted results were effective for compound optimization, the constructed models were complex, and it was difficult to elucidate their relationships. Shapley additive explanations (SHAP) is a useful tool to overcome this hurdle, where the method calculates an important value of each feature for a prediction based on game theory. The calculation performed by the method was time- and memory-intensive due to which we did not compute the SHAP in Papp dataset. However, the script to calculate SHAP value is shown in supplemental information for Papp prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce023f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JS visualization code to notebook\n",
    "shap.initjs()\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.KernelExplainer(svr_model.predict, pd.DataFrame(X_test_elastic))\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_test_elastic)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test_elastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378aaf3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- J-Stage: https://www.jstage.jst.go.jp/article/ciqs/2016/0/2016_Y4/_pdf/-char/ja\n",
    "- github: https://github.com/mordred-descriptor/mordred\n",
    "- kiseno-log: https://kiseno-log.com/2019/11/07/mordred%E3%81%A7%E8%A8%98%E8%BF%B0%E5%AD%90%E3%82%92%E8%A8%88%E7%AE%97%E3%81%97%E3%81%A6pandas%E5%BD%A2%E5%BC%8F%E3%81%A7%E5%87%BA%E5%8A%9B%E3%81%99%E3%82%8B/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
